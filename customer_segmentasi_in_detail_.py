# -*- coding: utf-8 -*-
"""Customer Segmentasi in detail .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EUeVIu8iRspIa7pwkWvDEuoFKsvuLtf1
"""

import pandas as pd
pd.set_option('display.max_columns',None)
pd.set_option('display.max_rows',None)
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

import pandas as pd

# Load semua file
users = pd.read_csv("users.csv")
orders = pd.read_csv("orders.csv")
order_items = pd.read_csv("order_items.csv")
products = pd.read_csv("products.csv")

# Rename kolom id di products -> product_id
products = products.rename(columns={"id": "product_id"})

# Gabungkan orders dengan users
df = orders.merge(users, left_on="user_id", right_on="id", how="left")

# Gabungkan dengan order_items
df = df.merge(order_items, on="order_id", how="left")

# Gabungkan dengan products (untuk dapat kategori, harga, dll)
df = df.merge(products, on="product_id", how="left")

print(df.head())

df.head()

"""di sini ada data pribadi user yang dimulai dari email"""

df.info()

"""dari sini nilai sold_atnya lebih dari setengah dari total yang hilang
disini dlivery_at_y ini banyak NAN ini karena data ini ada yang cancel makanya banyak yang berisi nan.

"""

# Buat copy agar aman
df_clean = df[[
    "order_id",
    "user_id_x",        # dari orders
    "created_at_x",     # order date
    "sale_price",       # harga jual per produk
    "product_id",
    "category",
    "brand",
    "retail_price",
    "department",
    "first_name",
    "last_name",
    "email",
    "age",
    "gender_x",         # dari users
    "city",
    "country"
]].rename(columns={
    "user_id_x": "user_id",
    "created_at_x": "order_date",
    "gender_x": "gender"
})

print(df_clean.head())

df_clean.info()

df['gender_y'].unique()

"""karena gender diambil dari data user dan order id dari data set order makanya banyak sekali nan maka saya akan menghendling nan ini dengan pendekatan global yaitu median dan modus"""

df['order_id'].nunique()

"""dari sini terlihat bahwa ada yang double sehingga bisa dilakukan RFM"""

# Pastikan order_date dalam datetime
df_clean["order_date"] = pd.to_datetime(df_clean["order_date"], errors="coerce")

# Drop user yang tidak punya order_date (belum pernah order)
raw_data = df_clean.dropna(subset=["order_date", "sale_price"]).copy()

raw_data.info()

raw_data['brand']=raw_data['brand'].fillna(raw_data['brand'].mode()[0])
raw_data['city']=raw_data['city'].fillna(raw_data['city'].mode()[0])

raw_data.info()

raw_data.duplicated().sum()

raw_data.drop_duplicates(inplace=True)

raw_data.duplicated().sum()

raw_data.isnull().sum()

raw_data.info()

raw_data['order_date'].nunique()

"""karena ada double maka  bisa dilakukan rfm"""

for col in raw_data.columns:
    value_counts = raw_data[col].value_counts()
    print(f"=== {col} ===")
    print(f"Jumlah value counts: {len(value_counts)}")  # Jumlah value counts
    print(value_counts.head(50))  # Isi value counts
    print("-" * 50)

"""#distibusi data"""

#mengecek seluruh column distribusi data numeric
for col in raw_data.select_dtypes(include = 'number').columns:
  plt.figure(figsize = (5,5))
  sns.histplot(raw_data[col], kde = True)
  plt.title(f'Distribution of {col}')
  plt.show()

#mengecek seluruh outliers pada seluruh column numeric
for col in raw_data.select_dtypes(include='number').columns:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x=raw_data[col])
    plt.title(f'Boxplot of {col}')
    plt.show()

"""karena banyak outliers pada column retail price dan sale price mungkin akan dipertimbangkan untuk dilakukan headling outliers


"""

#df copy untuk melihat apakah ini sudah di clean dengan benar shingga data aslinya tidak terganti jika ini eror
def remove_outliers_iqr(df):
    df_clean = raw_data.copy()
    for col in df_clean.select_dtypes(include='number').columns:
        Q1 = df_clean[col].quantile(0.25)
        Q3 = df_clean[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        df_clean = df_clean[(df_clean[col] >= lower) & (df_clean[col] <= upper)]
    return df_clean

df_no_outliers = remove_outliers_iqr(df)

# Visualisasi boxplot semua kolom numerik tanpa outlier
for col in df_no_outliers.select_dtypes(include='number').columns:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x=df_no_outliers[col].dropna())
    plt.title(f'Boxplot of {col}')
    plt.xlabel(col)
    plt.show()

df_clean.info()

# --- 4. Filter data valid (order_date & sale_price tidak null)
df_rfm = df_clean.dropna(subset=["order_date", "sale_price"]).copy()

# --- 5. Hitung nilai RFM per user
import datetime as dt

# tentukan tanggal acuan (misalnya max tanggal order + 1 hari)
ref_date = df_rfm['order_date'].max() + pd.Timedelta(days=1)

rfm = df_rfm.groupby('user_id').agg({
    'order_date': lambda x: (ref_date - x.max()).days,  # Recency
    'order_id': 'count',                                # Frequency
    'sale_price': 'sum'                                 # Monetary
}).reset_index()

rfm.columns = ['user_id', 'Recency', 'Frequency', 'Monetary']

# --- 6. Skor RFM (1 = rendah, 5 = tinggi)
rfm['R_Score'] = pd.qcut(rfm['Recency'].rank(method='first'), 5, labels=[5,4,3,2,1])
rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])
rfm['M_Score'] = pd.qcut(rfm['Monetary'].rank(method='first'), 5, labels=[1,2,3,4,5])




# Gabungkan jadi satu segmen
rfm['RFM_Segment'] = rfm['R_Score'].astype(str) + rfm['F_Score'].astype(str) + rfm['M_Score'].astype(str)
rfm['RFM_Score'] = rfm[['R_Score','F_Score','M_Score']].astype(int).sum(axis=1)

# --- 7. Tentukan Kategori Segmentasi
def segment(x):
    if x >= 12:
        return 'Loyal Customer'
    elif x >= 9:
        return 'Potential Loyalist'
    elif x >= 6:
        return 'Needs Attention'
    else:
        return 'At Risk'

rfm['Segment'] = rfm['RFM_Score'].apply(segment)

# --- 8. Lihat hasil
print(rfm.head(10))

# --- 9. Simpan hasil untuk Power BI
rfm.to_csv("rfm_segmented.csv", index=False)

# Gabungkan hasil RFM ke data awal
df_final = raw_data.merge(
    rfm[['user_id','Recency','Frequency','Monetary','RFM_Score','Segment']],
    on='user_id',
    how='left'
)

# Simpan hasil untuk Power BI
df_final.to_csv("rfm_with_raw.csv", index=False)

# --- Distribusi Segmen
plt.figure(figsize=(6,4))
sns.countplot(data=rfm, x='Segment', order=rfm['Segment'].value_counts().index, palette='viridis')
plt.title("Distribusi Customer per Segment")
plt.xlabel("Segment")
plt.ylabel("Jumlah Customer")
plt.show()

plt.figure(figsize=(6,4))
sns.barplot(data=rfm, x='Segment', y='Monetary', order=rfm['Segment'].value_counts().index, palette='coolwarm')
plt.title("Rata-rata Monetary per Segment")
plt.xlabel("Segment")
plt.ylabel("Rata-rata Monetary")
plt.show()

plt.figure(figsize=(6,5))
sns.scatterplot(data=rfm, x='Recency', y='Frequency', hue='Segment', alpha=0.7, palette='tab10')
plt.title("Recency vs Frequency by Segment")
plt.show()

# Jumlah loyal customer per kota
city_segment = df_final[df_final['Segment']=='Loyal Customer'].groupby('city')['user_id'].nunique().sort_values(ascending=False).head(10)

plt.figure(figsize=(8,5))
city_segment.plot(kind='bar', color='skyblue')
plt.title("Top 10 Kota dengan Loyal Customer Terbanyak")
plt.xlabel("City")
plt.ylabel("Jumlah Loyal Customer")
plt.show()

import matplotlib.pyplot as plt

# Hitung discount rate
df_clean['discount_rate'] = (df_clean['retail_price'] - df_clean['sale_price']) / df_clean['retail_price'] * 100

# Hapus nilai yang tidak valid (misal retail_price = 0)
df_clean = df_clean[df_clean['retail_price'] > 0]

# Plot distribusi discount rate
plt.figure(figsize=(8,5))
plt.hist(df_clean['discount_rate'], bins=50, edgecolor='k', alpha=0.7)
plt.title("Distribusi Discount Rate (%)")
plt.xlabel("Discount Rate (%)")
plt.ylabel("Frekuensi")
plt.show()

# Ringkasan statistik
print(df_clean['discount_rate'].describe())

"""karena ini 0.0 maka tidak ada discount pada data set ini."""

# Cek apakah semua nilai retail_price sama dengan sale_price
print((df_clean['retail_price'] == df_clean['sale_price']).all())

# Hitung berapa persen yang berbeda
diff_pct = (df_clean['retail_price'] != df_clean['sale_price']).mean() * 100
print(f"Persentase transaksi dengan harga berbeda: {diff_pct:.2f}%")

import matplotlib.pyplot as plt
import seaborn as sns

# Pastikan age numeric
df_clean['age'] = pd.to_numeric(df_clean['age'], errors='coerce')

# Bikin age group biar lebih mudah segmentasi
bins = [0, 17, 25, 35, 50, 65, 100]
labels = ['<18', '18-25', '26-35', '36-50', '51-65', '65+']
df_clean['age_group'] = pd.cut(df_clean['age'], bins=bins, labels=labels, right=False)

# --- 1. Distribusi Gender ---
plt.figure(figsize=(6,4))
sns.countplot(data=df_clean, x='gender', palette="Set2")
plt.title("Distribusi Customer Berdasarkan Gender")
plt.xlabel("Gender")
plt.ylabel("Jumlah")
plt.show()

# --- 2. Distribusi Usia ---
plt.figure(figsize=(8,4))
sns.histplot(df_clean['age'], bins=30, kde=True, color="skyblue")
plt.title("Distribusi Usia Customer")
plt.xlabel("Usia")
plt.ylabel("Jumlah")
plt.show()

# --- 3. Segmentasi Gender x Age Group ---
plt.figure(figsize=(8,5))
sns.countplot(data=df_clean, x='age_group', hue='gender', palette="Set1")
plt.title("Segmentasi Customer berdasarkan Usia & Gender")
plt.xlabel("Kelompok Usia")
plt.ylabel("Jumlah Customer")
plt.legend(title="Gender")
plt.show()

# --- 1. Cari brand yang paling banyak dibeli ---
top_brand = (
    df_clean.groupby("brand")['order_id']
    .count()
    .reset_index()
    .rename(columns={'order_id':'total_order'})
    .sort_values(by='total_order', ascending=False)
)

print("Top 10 brand yang paling banyak dibeli:")
print(top_brand.head(10))

# --- 2. Visualisasi Top 10 Brand ---
plt.figure(figsize=(10,5))
sns.barplot(data=top_brand.head(10), x='total_order', y='brand', palette="Blues_r")
plt.title("Top 10 Brand Paling Banyak Dibeli")
plt.xlabel("Jumlah Order")
plt.ylabel("Brand")
plt.show()

# --- 3. Kota asal brand terlaris ---
# Ambil brand teratas
best_brand = top_brand.iloc[0]['brand']

brand_city = (
    df_clean[df_clean['brand'] == best_brand]
    .groupby('city')['order_id']
    .count()
    .reset_index()
    .rename(columns={'order_id':'total_order'})
    .sort_values(by='total_order', ascending=False)
)

print(f"Kota dengan pembelian terbanyak untuk brand {best_brand}:")
print(brand_city.head(10))

# --- 4. Visualisasi ---
plt.figure(figsize=(10,5))
sns.barplot(data=brand_city.head(10), x='total_order', y='city', palette="viridis")
plt.title(f"Top 10 Kota dengan Order Terbanyak untuk Brand {best_brand}")
plt.xlabel("Jumlah Order")
plt.ylabel("Kota")
plt.show()

# Gabungkan RFM (ada Segment) dengan df_clean (ada city)
df_rfm_city = rfm.merge(
    df_clean[['user_id', 'city']].drop_duplicates(),
    on='user_id',
    how='left'
)

# Hitung jumlah pelanggan tiap Segment di setiap kota
loyalty_by_city = (
    df_rfm_city.groupby(['city', 'Segment'])['user_id']
    .nunique()
    .reset_index()
    .rename(columns={'user_id': 'customer_count'})
)

print(loyalty_by_city.head(10))

import matplotlib.pyplot as plt

top_loyal_city = (
    loyalty_by_city[loyalty_by_city['Segment'] == 'Loyal Customer']
    .sort_values(by='customer_count', ascending=False)
    .head(5)
)

plt.figure(figsize=(8,5))
plt.bar(top_loyal_city['city'], top_loyal_city['customer_count'], color='skyblue')
plt.title("Top 5 Kota dengan Loyal Customer Terbanyak")
plt.xlabel("Kota")
plt.ylabel("Jumlah Loyal Customer")
plt.xticks(rotation=45)
plt.show()

print(rfm.columns)

# Filter hanya Segment = 'At Risk'
at_risk_city = (
    df_rfm_city[df_rfm_city['Segment'] == 'At Risk']
    .groupby('city')['user_id']
    .nunique()
    .reset_index()
    .rename(columns={'user_id': 'customer_count'})
    .sort_values(by='customer_count', ascending=False)
    .head(5)
)

print(at_risk_city)

top_brand = (
    df_clean.groupby('brand')['user_id']
    .nunique()
    .reset_index()
    .sort_values(by='user_id', ascending=False)
    .head(1)  # ambil brand paling atas
)
print(top_brand)

# Ambil brand teratas
top_brand_name = top_brand.iloc[0]['brand']

# Gabungkan brand ke RFM
df_rfm_brand = rfm.merge(
    df_clean[['user_id','brand']].drop_duplicates(),
    on='user_id',
    how='left'
)

# Hitung distribusi segmentasi untuk brand teratas
brand_segment = (
    df_rfm_brand[df_rfm_brand['brand'] == top_brand_name]
    .groupby('Segment')['user_id']
    .nunique()
    .reset_index()
    .rename(columns={'user_id':'customer_count'})
    .sort_values(by='customer_count', ascending=False)
)

print(brand_segment)

plt.figure(figsize=(6,6))
plt.pie(
    brand_segment['customer_count'],
    labels=brand_segment['Segment'],
    autopct='%1.1f%%',
    startangle=140,
    colors=plt.cm.Paired.colors
)
plt.title(f"Distribusi Segmentasi untuk Brand {top_brand_name}")
plt.show()

# Gabungkan RFM dengan data brand
df_rfm_brand = rfm.merge(
    df_clean[['user_id','brand']].drop_duplicates(),
    on='user_id',
    how='left'
)

# Filter hanya At Risk
at_risk_brand = (
    df_rfm_brand[df_rfm_brand['Segment'] == 'At Risk']
    .groupby('brand')['user_id']
    .nunique()
    .reset_index()
    .rename(columns={'user_id':'customer_count'})
    .sort_values(by='customer_count', ascending=False)
)

print(at_risk_brand.head(10))  # Top 10 brand At Risk

top5_at_risk_brand = at_risk_brand.head(5)

plt.figure(figsize=(8,5))
plt.bar(top5_at_risk_brand['brand'], top5_at_risk_brand['customer_count'], color='orange')
plt.title("Top 5 Brand yang Dibeli Pelanggan 'At Risk'")
plt.xlabel("Brand")
plt.ylabel("Jumlah Customer At Risk")
plt.xticks(rotation=45)
plt.show()

# Gabungkan RFM dengan data city
df_rfm_city = rfm.merge(
    df_clean[['user_id','city']].drop_duplicates(),
    on='user_id',
    how='left'
)

# Filter hanya At Risk
at_risk_city = (
    df_rfm_city[df_rfm_city['Segment'] == 'At Risk']
    .groupby('city')['user_id']
    .nunique()
    .reset_index()
    .rename(columns={'user_id':'customer_count'})
    .sort_values(by='customer_count', ascending=False)
)

print(at_risk_city.head(5))  # Top 5 Kota At Risk

# Gabungkan RFM dengan city dan brand
df_rfm_city_brand = rfm.merge(
    df_clean[['user_id','city','brand']].drop_duplicates(),
    on='user_id',
    how='left'
)

# Filter untuk At Risk di kota Dongguan
dongguan_at_risk = df_rfm_city_brand[
    (df_rfm_city_brand['Segment'] == 'At Risk') &
    (df_rfm_city_brand['city'] == 'Dongguan')
]

# Hitung top brand di Dongguan
top_brand_dongguan = (
    dongguan_at_risk.groupby('brand')['user_id']
    .nunique()
    .reset_index()
    .rename(columns={'user_id':'customer_count'})
    .sort_values(by='customer_count', ascending=False)
)

print(top_brand_dongguan.head(10))  # Top 10 brand di Dongguan

plt.figure(figsize=(8,5))
plt.barh(top_brand_dongguan.head(10)['brand'],
         top_brand_dongguan.head(10)['customer_count'],
         color='orange')
plt.title("Top 10 Brand di Kota Dongguan (Segment At Risk)")
plt.xlabel("Jumlah Customer At Risk")
plt.ylabel("Brand")
plt.gca().invert_yaxis()
plt.show()

df_clean.info()

# Ambil order terakhir setiap user
last_order = df_clean.groupby("user_id")["order_date"].max().reset_index()
last_order.rename(columns={"order_date": "last_order_date"}, inplace=True)

# Cari tanggal paling akhir di dataset
max_date = df_clean["order_date"].max()

# Hitung hari sejak order terakhir
last_order["days_since_last_order"] = (max_date - last_order["last_order_date"]).dt.days

# Definisikan churn jika lebih dari 90 hari tidak belanja
cutoff_days = 90
last_order["churn"] = (last_order["days_since_last_order"] > cutoff_days).astype(int)

df_clean = df_clean.merge(last_order[["user_id", "churn"]], on="user_id", how="left")

df_clean.head()

df_clean.info()

df_clean['department'].unique()

"""#Label Encoder"""

df_clean.set_index("order_id", inplace = True)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df_clean["category"] = le.fit_transform(df_clean["category"])
df_clean["brand"] = le.fit_transform(df_clean["brand"])
df_clean["department"] = le.fit_transform(df_clean["department"])
df_clean["gender"] = le.fit_transform(df_clean["gender"])
df_clean["city"] = le.fit_transform(df_clean["city"])
df_clean["country"] = le.fit_transform(df_clean["country"])
df_clean["age_group"] = le.fit_transform(df_clean["age_group"])

df_clean['age'].nunique()

X = df_clean.drop(columns=["churn", "email", "first_name", "last_name", "order_date", "age_group"])
y = df_clean["churn"]

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size= 0.2,
    stratify = y,
    random_state = 2000
)

X_train.info()

X_train["churn"] = y_train

X_train.info()

y.value_counts()

corr = X_train.corr()

masking = np.triu(np.ones_like(corr, dtype=bool))

f,ax = plt.subplots(figsize=(20, 20))

cmap = sns.diverging_palette(230, 20, as_cmap = True)

sns.heatmap(
    corr,
    mask=masking,
    cmap=cmap,
    vmax= .3,
    center=0,
    square= True,
    linewidths= .5,
    annot = True,
    annot_kws={"size": 12}
)

"""ada hubungan kuat antara departement dengan product id yang mana ini bernilai negatif, sedangkan product id sangat berpengaruh positif dengan gender

#data category vs churn
"""

def prop_agg(df1, y, x):
  temp_df1 = df1.groupby([y,x], as_index=False).size()
  temp_df1['prop'] = temp_df1['size']/ temp_df1.groupby(y)['size'].transform('sum')
  return temp_df1

prop_agg(X_train, "gender", "churn")

"""#Gender VS Churn"""

def facet_barplot(data, feats, x_label, y_label ):
  '''
  data : DataFrame tahat include "gender", x_label, y_label
  x_label : column name to use for x-axis and hue
  y_label : column name to use for y-axis
  '''

  unique_x = data[x_label].unique()
  palette = sns.color_palette("Set2", len(unique_x))

  g = sns.FacetGrid(data=data, col=feats, sharey=True)

  g.map_dataframe(
      sns.barplot,
      x=x_label,
      y=y_label,
      hue=x_label,
      palette=palette,
      order=sorted(unique_x)
  )
  for ax in g.axes.flat:
    for p in ax.patches:
      height = p.get_height()
      ax.text(
          p.get_x() + p.get_width()/2,
          height,
          f'{height: .2f}',
          ha="center",
          va ="bottom"
      )
      g.add_legend()
      plt.tight_layout()
      plt.show()
      plt.close()

feature = "gender"

df=prop_agg(X_train, feature, "churn")

facet_barplot(
    data=df,
    feats=feature,
    x_label="churn",
    y_label="prop"
)



"""##category vs churn"""

feature = "category"

df=prop_agg(X_train, feature, "churn")

facet_barplot(
    data=df,
    feats=feature,
    x_label="churn",
    y_label="prop"
)

"""###Churn vs brand"""

feature = "brand"

df=prop_agg(X_train, feature, "churn")

facet_barplot(
    data=df,
    feats=feature,
    x_label="churn",
    y_label="prop"
)

df_clean['brand'].unique()

"""karena column brand sangat banyak sehingga ini menyebabkan overfritting jadi akan saya tanyakan ke stakeholder apakah bisa dikelompok brandnya atau tidak

###churn vs department
"""

feature = "department"

df=prop_agg(X_train, feature, "churn")

facet_barplot(
    data=df,
    feats=feature,
    x_label="churn",
    y_label="prop"
)

"""###churn vs city"""

feature = "city"

df=prop_agg(X_train, feature, "churn")

facet_barplot(
    data=df,
    feats=feature,
    x_label="churn",
    y_label="prop"
)

df_clean['city'].nunique()

"""dari sini juga banyak banget makanya bisa overfriting mungkin akan difokuskan ke country

###churn vs country
"""

feature = "country"

df=prop_agg(X_train, feature, "churn")

facet_barplot(
    data=df,
    feats=feature,
    x_label="churn",
    y_label="prop"
)

df_clean['country'].nunique()

"""##Data numeric VS Churn

###sale_price Vs churn
"""

sns.boxplot(x ="churn", y = "sale_price", data = X_train)

X_train.columns[X_train.columns.duplicated()]

"""banyak sekali outliers dari sini antara churn dan tidak churn

###product_id vs Churn
"""

sns.boxplot(x = "churn", y = "product_id", data = X_train)

"""dari sini tidak ada outliers

###retail_price vs chrun
"""

sns.boxplot(x = "churn", y = "retail_price", data = X_train)

"""ada outliers ini normal karena bisa jadi ada customer yang membeli lebih dari rata rata tetapi ini bisa membuat model KNN akan sulit untuk membaca jadi akan saya skip untuk menggunakan model KNN dan jika menggunakan knn harus digunakan smote agar model bisa membaca dengan sangat baik

##Modeling
"""

X = df_clean.drop(columns=["churn", "email", "first_name", "last_name", "order_date", "age_group"])
y = df_clean["churn"]

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size= 0.2,
    stratify = y,
    random_state = 2000
)

"""###Decision Tree"""

dc_clf = DecisionTreeClassifier(
    max_depth = 5,
    ccp_alpha = 0.001,
    class_weight= "balanced"
)

"""###Random Forest"""

rf_clf = RandomForestClassifier(
    random_state=2000,
    n_estimators=300,
    class_weight="balanced",
    n_jobs=-1   # gunakan semua core CPU
)

"""Tidak pakai n_estimator 2000 karena data set ini banyak sehingga akan saya kecilkan nilai estimator dan menambahkan n_jobs supaya makin cepat ketika modeling dan fit modelnya, karena n_estimator adalah berapa banyak pohon pada random forest yang dapat dibuat jika pohonnya banyak maka akan membuat overfriting pada saat fitting modelnya nanti

##SVM RBF
"""

from sklearn.svm import LinearSVC
from sklearn.calibration import CalibratedClassifierCV

base_svm = LinearSVC(random_state=2000)
svm = CalibratedClassifierCV(base_svm, cv=3)  # kalibrasi untuk dapat probabilitas
svm.fit(X_train, y_train)

"""ketika emnggunakan class eight dengan yang sama random state =2000 dia menjadi ovefriting jadi saya akan menggunakan kalibrasi pada tahap SVM ini agar komputer mudah untuk menganalasis kolom ini karena kolom ini terdiri dari 140.000an lebih

###XGBoost
"""

from xgboost import XGBClassifier
import numpy as np

# hitung imbalance ratio
neg, pos = np.bincount(y_train)
scale_pos_weight = neg / pos
print("scale_pos_weight:", scale_pos_weight)

xgb_clf = XGBClassifier(
    random_state=2000,
    n_estimators=2000,
    scale_pos_weight=scale_pos_weight
)

"""##Fiting model to data"""

#Diciison Tree

dc_clf.fit(X_train, y_train)

rf_clf.fit(X_train, y_train)

"""ketika menggunakan svm dan classweigtnya sama tetapi overfritting loadingnya lama sehingga disini saya akan mengambil calss weightnya sama /balance saja berlandaskan perbedaan dari kedua nilai ini yang cukup besar"""

xgb_clf.fit(X_train, y_train)

"""##Model evaluation"""

X_train.shape

X_train.info()

X_test.info()

X_test.shape

#decision tree
dc_pred = dc_clf.predict(X_test)
dc_pred_proba = dc_clf.predict_proba(X_test)

#random forest
rf_pred = rf_clf.predict(X_test)
rf_pred_proba = rf_clf.predict_proba(X_test)

#SVM
svm_pred = svm.predict(X_test)
svm_pred_proba = svm.predict_proba(X_test)

#XGBOOST
xgb_pred = xgb_clf.predict(X_test)
xgb_pred_proba = xgb_clf.predict_proba(X_test)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics

def plot_confusion_matrix(y_true, y_pred, labels = None, title = "Confusion Matrix"):
  cm = metrics.confusion_matrix(y_true, y_pred, labels=labels)

  plt.figure(figsize=(6,5))
  sns.heatmap(cm, annot = True, fmt = "d", cmap = "Blues",
              xticklabels = labels if labels else sorted(set(y_true)),
              yticklabels = labels if labels else sorted(set(y_true)))

  plt.xlabel("Predicted Labels")
  plt.ylabel("True Labels")
  plt.title(title)
  plt.show()

plot_confusion_matrix(y_test, dc_pred, labels = [0,1], title = "Decision Tree Confusion Matrix")

pd.DataFrame(metrics.classification_report(y_test, dc_pred, target_names =["Not Churn", "Churn"], output_dict = True))

"""model bias ke kelas mayoritas karena lebih banyak yang churn"""

plot_confusion_matrix(y_test, rf_pred, labels = [0,1], title = "Random Forest Confusion Matrix")

"""kalau menggunakan random forest ini terlihat FN, TNnya dibandingkan decision tree"""

pd.DataFrame(metrics.classification_report(y_test, rf_pred, target_names = ["Not Churn", "Churn"], output_dict = True))

"""dari sini model sudah bisa mendangkap dibandingkan menggunakan algoritma decision tree"""

plot_confusion_matrix(y_test, svm_pred, labels =[0,1], title = "SVM Confusion Matrix")

"""model gagal membedakan FN dan TN dari gambar ini"""

pd.DataFrame(metrics.classification_report(y_test, svm_pred, target_names = ["Not Churn", "Churn"], output_dict = True))

plot_confusion_matrix(y_test, xgb_pred, labels = [0,1], title = "Xgboost Confusion Matrix")

"""disini model lebih bagus dalam memprediksi churn dan no churnnya"""

pd.DataFrame(metrics.classification_report(y_test, xgb_pred, target_names = ['Not_Churn', 'Churn'], output_dict = True))

"""dari sini not churn dibawah churn berbeda dengan matrix random forest tetapi dalam FN dan TN lebih bagusan XGBoost dalam membaca model ini.

##Model Result

###Plot Roc dan Precision-Recall Curve
"""

from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import numpy as np

def plot_roc_curve(y_true, y_scores, title="ROC AUC Curve"):
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    auc_score = roc_auc_score(y_true, y_scores)

    plt.figure(figsize=(6,5))
    plt.plot(fpr, tpr, label=f"AUC = {auc_score:.2f}")
    plt.plot([0,1],[0,1], 'k--', linewidth=1)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

def plot_precision_recall_with_opt_threshold(y_true, y_scores, title="Precision Recall Curve"):
    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)
    thresholds = np.append(thresholds, 1.0)

    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
    best_index = np.argmax(f1_scores)
    best_threshold = thresholds[best_index]
    best_f1 = f1_scores[best_index]

    plt.figure(figsize=(6,5))
    plt.plot(recall, precision, label=f'AP = {average_precision_score(y_true, y_scores):.2f}')
    plt.scatter(recall[best_index], precision[best_index],
                color='red', zorder=10,
                label=f'Best F1 = {best_f1:.2f}\nThreshold = {best_threshold:.2f}')

    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    return best_threshold

y_proba = xgb_clf.predict_proba(X_test)[:,1]

best_thresh = plot_precision_recall_with_opt_threshold(y_test, y_proba)
plot_roc_curve(y_test, y_proba)

"""kita dapat menentukan bahwa ambang batas (threshold) yang optimal adalah 0,01. Mengapa kita membutuhkannya? Karena kita perlu mempertahankan pelanggan agar tidak churn dengan keterbatasan anggaran yang tersedia dan kemungkinan ini terjadi karena model ini gagal memprediksi jadi akan saya lakukan smote.

#Smote
"""

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state = 42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print("sebelum di smote:", pd.Series(y_train).value_counts)
print("Sesudah di Smote", pd.Series(y_train_smote).value_counts)

#KNN
knn_clf = KNeighborsClassifier(n_neighbors=5)
knn_clf.fit(X_train_smote, y_train_smote)
knn_pred = knn_clf.predict(X_test)
knn_pred_proba = knn_clf.predict_proba(X_test)

#DcisionTREE
dc_clf.fit(X_train_smote, y_train_smote)
dc_pred = dc_clf.predict(X_test)
dc_pred_proba = dc_clf.predict_proba(X_test)

#Random Forest
rf_clf.fit(X_train_smote, y_train_smote)
rf_pred = rf_clf.predict(X_test)
rf_pred_proba = rf_clf.predict_proba(X_test)

#XGBoost
xgb_clf.fit(X_train_smote, y_train_smote)
xgb_pred = xgb_clf.predict(X_test)
xgb_pred_proba = xgb_clf.predict_proba(X_test)

plot_confusion_matrix(y_test, knn_pred, labels=[0, 1], title="KNN Confusion Matrix")

pd.DataFrame(metrics.classification_report(y_test, knn_pred, target_names=['Not Churn','Churn'], output_dict=True))

plot_confusion_matrix(y_test, dc_pred, labels=[0, 1], title="Decision Tree Confusion Matrix")

pd.DataFrame(metrics.classification_report(y_test, dc_pred, target_names=['Not Churn','Churn'], output_dict=True))

plot_confusion_matrix(y_test, rf_pred, labels=[0, 1], title="Random Forest Confusion Matrix")

pd.DataFrame(metrics.classification_report(y_test, rf_pred, target_names=['Not Churn','Churn'], output_dict=True))

plot_confusion_matrix(y_test, rf_pred, labels=[0, 1], title="Random Forest Confusion Matrix")

pd.DataFrame(metrics.classification_report(y_test, rf_pred, target_names=['Not Churn','Churn'], output_dict=True))

plot_confusion_matrix(y_test, xgb_pred, labels=[0, 1], title="XGBoost Confusion Matrix")

pd.DataFrame(metrics.classification_report(y_test, rf_pred, target_names=['Not Churn','Churn'], output_dict=True))

pd.DataFrame(metrics.classification_report(y_test, xgb_pred, target_names=['Not Churn','Churn'], output_dict=True))

"""kalau kita lihat dari tingkat akurasi XGBoost masih menjadi model yang terbaik di angka 66,61%

#ROC dan AUC setelah di Smote
"""

from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, roc_auc_score
import matplotlib.pyplot as plt


def plot_roc_curve(y_true, y_scores, title="ROC AUC Curve"):
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    auc_score = roc_auc_score(y_true, y_scores)

    plt.figure(figsize=(6, 5))
    plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')
    plt.plot([0, 1], [0, 1], 'k--', linewidth=1)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()


def plot_precision_recall_with_opt_threshold(y_true, y_scores, title="Precision-Recall Curve"):
    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)
    thresholds = np.append(thresholds, 1.0)

    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
    best_index = np.argmax(f1_scores)
    best_threshold = thresholds[best_index]
    best_f1 = f1_scores[best_index]

    plt.figure(figsize=(6, 5))
    plt.plot(recall, precision, label=f'AP = {average_precision_score(y_true, y_scores):.2f}')
    plt.scatter(recall[best_index], precision[best_index],
                color='red', zorder=10,
                label=f'Best F1 = {best_f1:.2f}\nThreshold = {best_threshold:.2f}')

    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    return best_threshold

y_proba = xgb_clf.predict_proba(X_test)[:, 1]

best_thresh = plot_precision_recall_with_opt_threshold(y_test, y_proba)
plot_roc_curve(y_test, y_proba)

"""dari sini terlihat tingkat thresholdny jadi berubah menjadi 0.14 dan untuk AUCnya di angka 70% tidak lagi mendekati netral 50%

##Perioritas Customer
"""

df_clean['probability_churn'] = xgb_clf.predict_proba(X)[:,1]
df_clean['predicted_churn'] = xgb_clf.predict(X)

df_clean[df_clean['probability_churn']>=0.28].shape

df_churn = df_clean[df_clean['probability_churn']>=0.28]
df_churn.head(20)

df_churn.sort_values('sale_price', ascending = False)

